{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def crawl_with_playwright(url: str):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(url, wait_until=\"networkidle\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "        return content\n",
    "    \n",
    "def extract_xml_from_html(html: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Check for any embedded XML-looking structure\n",
    "    xml_tags = [\"sitemapindex\", \"urlset\", \"feed\", \"rss\"]  # common root XML tags\n",
    "    for tag in xml_tags:\n",
    "        if soup.find(tag):\n",
    "            return str(soup.find(tag))\n",
    "        \n",
    "    print(\"‚ùå Couldn't find XML in the HTML.\")\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sitemap for https://www.westside.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a6c7e50>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'# ')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a215fd0>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a162250>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x105afa2d0>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a6b1550>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a5725d0>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10ac492d0>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a2aff50>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x1055545d0>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a10c150>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x105d37cd0>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<?')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x105447850>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<h')\n",
      "Parsing sitemap from URL https://my-westside.myshopify.com/admin/auth/login failed: Unsupported root element 'html'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sitemap for https://virgio.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a6de690>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'# ')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x10a643350>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'\\n ')\n",
      "Unable to gunzip response <usp.web_client.requests_client.RequestsWebClientSuccessResponse object at 0x105d27390>, maybe it's a non-gzipped sitemap: Unable to gunzip data: Not a gzipped file (b'<h')\n",
      "Parsing sitemap from URL https://virgiofashion.myshopify.com/admin/auth/login failed: Unsupported root element 'html'.\n"
     ]
    }
   ],
   "source": [
    "from usp.tree import sitemap_tree_for_homepage, InvalidSitemap, sitemap_from_str\n",
    "from collections import deque\n",
    "\n",
    "list_of_websites = [\n",
    "    \"https://www.westside.com/\",\n",
    "    \"https://virgio.com/\",\n",
    "    \"https://www.tatacliq.com/\",\n",
    "    \"https://nykaafashion.com/\"\n",
    "]\n",
    "all_urls = []\n",
    "for website in list_of_websites:\n",
    "    print(f\"Fetching sitemap for {website}\")\n",
    "    tree = sitemap_tree_for_homepage(website)\n",
    "\n",
    "    count = 0\n",
    "    for page in tree.all_pages():\n",
    "        all_urls.append(page.url)\n",
    "        count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        print(\" No URLs found in the sitemap. Maybe something failed!\")\n",
    "        for map in tree.all_sitemaps():\n",
    "            if isinstance(map, InvalidSitemap):\n",
    "                print('Invalid Sitemap:', map)\n",
    "                print(' 403 ' in getattr(map, 'reason', ''))\n",
    "                url_queue = deque([map.url])\n",
    "                while(len(url_queue) > 0):\n",
    "\n",
    "                    # Gather a batch of URLs to crawl at once (e.g., 5 at a time)\n",
    "                    batch_size = 5\n",
    "                    batch = []\n",
    "                    while len(url_queue)>0 and len(batch) < batch_size:\n",
    "                        url = url_queue.pop()\n",
    "                        batch.append(url)\n",
    "                        print('Fetching:', url)\n",
    "\n",
    "                    # Use asyncio to crawl multiple URLs in parallel\n",
    "                    async def fetch_and_process(urls):\n",
    "                        tasks = [crawl_with_playwright(u) for u in urls]\n",
    "                        results = await asyncio.gather(*tasks)\n",
    "                        return results\n",
    "                    xml_contents = await fetch_and_process(batch)\n",
    "\n",
    "                    # Process the fetched HTML content\n",
    "                    for xml_content in xml_contents:\n",
    "                        xml_str = extract_xml_from_html(xml_content)\n",
    "                        if xml_str:\n",
    "                            new_tree = sitemap_from_str(xml_str)\n",
    "                            for da in new_tree.all_pages():\n",
    "                                all_urls.append(da.url)\n",
    "                            for da in new_tree.all_sitemaps():\n",
    "                                url_queue.append(da.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import json\n",
    "\n",
    "parsed_links = []\n",
    "count = 0\n",
    "for u in all_urls:\n",
    "    parsed = urlparse(u)\n",
    "    path = parsed.path\n",
    "    split = parsed.path.split('/')\n",
    "    product = 0\n",
    "    data = {\n",
    "        \"url\": path,\n",
    "        \"label\": 0\n",
    "    }\n",
    "    if 'products' in split or 'p' in split:\n",
    "        data[\"label\"] = 1\n",
    "        count += 1\n",
    "    if product == 1 and len(parsed_links) > 300:\n",
    "        continue\n",
    "    parsed_links.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved parsed_links to dataset.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset.json\", \"w\") as outfile:\n",
    "    json.dump(parsed_links, outfile, indent=2)\n",
    "print(\"Saved parsed_links to dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
