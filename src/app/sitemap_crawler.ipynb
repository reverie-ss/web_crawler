{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def crawl_with_playwright(url: str):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(url, wait_until=\"networkidle\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "        return content\n",
    "    \n",
    "def extract_xml_from_html(html: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Check for any embedded XML-looking structure\n",
    "    xml_tags = [\"sitemapindex\", \"urlset\", \"feed\", \"rss\"]  # common root XML tags\n",
    "    for tag in xml_tags:\n",
    "        if soup.find(tag):\n",
    "            return str(soup.find(tag))\n",
    "        \n",
    "    print(\"âŒ Couldn't find XML in the HTML.\")\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from usp.tree import sitemap_tree_for_homepage, InvalidSitemap, sitemap_from_str\n",
    "from collections import deque\n",
    "\n",
    "list_of_websites = [\n",
    "    \"https://www.westside.com/\",\n",
    "    \"https://virgio.com/\",\n",
    "    \"https://www.tatacliq.com/\",\n",
    "    \"https://nykaafashion.com/\"\n",
    "]\n",
    "all_urls = []\n",
    "for website in list_of_websites:\n",
    "    print(f\"Fetching sitemap for {website}\")\n",
    "    tree = sitemap_tree_for_homepage(website)\n",
    "\n",
    "    count = 0\n",
    "    for page in tree.all_pages():\n",
    "        all_urls.append(page.url)\n",
    "        count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        print(\" No URLs found in the sitemap. Maybe something failed!\")\n",
    "        for map in tree.all_sitemaps():\n",
    "            if isinstance(map, InvalidSitemap):\n",
    "                print('Invalid Sitemap:', map)\n",
    "                print(' 403 ' in getattr(map, 'reason', ''))\n",
    "                url_queue = deque([map.url])\n",
    "                while(len(url_queue) > 0):\n",
    "\n",
    "                    # Gather a batch of URLs to crawl at once (e.g., 5 at a time)\n",
    "                    batch_size = 5\n",
    "                    batch = []\n",
    "                    while len(url_queue)>0 and len(batch) < batch_size:\n",
    "                        url = url_queue.pop()\n",
    "                        batch.append(url)\n",
    "                        print('Fetching:', url)\n",
    "\n",
    "                    # Use asyncio to crawl multiple URLs in parallel\n",
    "                    async def fetch_and_process(urls):\n",
    "                        tasks = [crawl_with_playwright(u) for u in urls]\n",
    "                        results = await asyncio.gather(*tasks)\n",
    "                        return results\n",
    "                    xml_contents = await fetch_and_process(batch)\n",
    "\n",
    "                    # Process the fetched HTML content\n",
    "                    for xml_content in xml_contents:\n",
    "                        xml_str = extract_xml_from_html(xml_content)\n",
    "                        if xml_str:\n",
    "                            new_tree = sitemap_from_str(xml_str)\n",
    "                            for da in new_tree.all_pages():\n",
    "                                all_urls.append(da.url)\n",
    "                            for da in new_tree.all_sitemaps():\n",
    "                                url_queue.append(da.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828292"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_PATH = \"../model/model.pkl\"\n",
    "VECTORIZER_PATH = \"../model/tfidf_vectorizer.pkl\"\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "\n",
    "def preprocess_url(url: str) -> str:\n",
    "    return url.lower().replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"-\", \" \").replace(\"/\", \" \")\n",
    "\n",
    "def predict_url(url: str) -> dict:\n",
    "    processed = preprocess_url(url)\n",
    "    vectorized = vectorizer.transform([processed])\n",
    "    pred = model.predict(vectorized)[0]\n",
    "    proba = model.predict_proba(vectorized)[0][pred]\n",
    "    return {\"label\": int(pred), \"confidence\": round(float(proba), 3)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_product_urls = []\n",
    "product_urls = []\n",
    "for url in all_urls:\n",
    "    prediction = predict_url(url)\n",
    "    if prediction[\"label\"] == 1:\n",
    "        product_urls.append(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"label\": prediction[\"label\"],\n",
    "                \"confidence\": prediction[\"confidence\"],\n",
    "                \"domain\": url.split(\"/\")[2],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"This URL is not a product page:\", url, prediction)\n",
    "        non_product_urls.append(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"label\": prediction[\"label\"],\n",
    "                \"confidence\": prediction[\"confidence\"],\n",
    "                \"domain\": url.split(\"/\")[2],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 824761 product URLs to product_urls.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"product_urls.json\", \"w\") as f:\n",
    "    json.dump(product_urls, f, indent=2)\n",
    "print(f\"Saved {len(product_urls)} product URLs to product_urls.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------below for creating dataset---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import json\n",
    "\n",
    "parsed_links = []\n",
    "count = 0\n",
    "for u in all_urls:\n",
    "    parsed = urlparse(u)\n",
    "    path = parsed.path\n",
    "    split = parsed.path.split('/')\n",
    "    product = 0\n",
    "    data = {\n",
    "        \"url\": path,\n",
    "        \"label\": 0\n",
    "    }\n",
    "    if 'products' in split or 'p' in split:\n",
    "        data[\"label\"] = 1\n",
    "        count += 1\n",
    "    parsed_links.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved parsed_links to dataset.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset.json\", \"w\") as outfile:\n",
    "    json.dump(parsed_links, outfile, indent=2)\n",
    "print(\"Saved parsed_links to dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
