{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_proxies(path=\"proxies.txt\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[~] Crawling: https://www.tatacliq.com/mens-clothing-casual-wear-t-shirts-polos/c-msh1116100\n",
      "[!] Fetch failed (1/3) https://www.tatacliq.com/mens-clothing-casual-wear-t-shirts-polos/c-msh1116100: Blocked with status 403\n",
      "[!] Fetch failed (2/3) https://www.tatacliq.com/mens-clothing-casual-wear-t-shirts-polos/c-msh1116100: Blocked with status 403\n",
      "[!] Fetch failed (3/3) https://www.tatacliq.com/mens-clothing-casual-wear-t-shirts-polos/c-msh1116100: Blocked with status 403\n",
      "\n",
      "✅ Done! Product URLs saved to 'product_urls.json'.\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Initial seed domains\n",
    "SEED_DOMAINS = [\n",
    "    # \"https:#www.virgio.com\",\n",
    "    # \"https:#www.tatacliq.com\",\n",
    "    # \"https:#www.nykaafashion.com\",\n",
    "    # \"https:#www.westside.com/collections/polo-t-shirts-for-men\",\n",
    "    \"https://www.tatacliq.com/mens-clothing-casual-wear-t-shirts-polos/c-msh1116100\"\n",
    "]\n",
    "\n",
    "# Product URL patterns\n",
    "PRODUCT_PATTERNS = [r'/product/', r'/p/', r'/item/', r'/shop/', r'/details/', r'/sku/']\n",
    "\n",
    "# Max pages to crawl per domain to avoid infinite crawling\n",
    "MAX_PAGES = 1000\n",
    "HEADERS = {\n",
    "    \"User-Agent\": random.choice([\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"CrawltonBot/1.0 (+https:#github.com/yourgithub/crawlton)\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Optional: list of proxies (use real proxies here)\n",
    "PROXIES = load_proxies()\n",
    "\n",
    "# Storage for output\n",
    "results = {}\n",
    "\n",
    "import random\n",
    "import asyncio\n",
    "\n",
    "# List of real user agents\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    # Add more if you like\n",
    "]\n",
    "\n",
    "# Optional: list of proxies (use real proxies here)\n",
    "PROXIES = [\n",
    "    # \"http:#user:pass@proxy1.com:port\",\n",
    "    # \"http:#proxy2.com:port\",\n",
    "]\n",
    "\n",
    "async def fetch(session, url, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Googlebot\"\n",
    "            }\n",
    "            # proxy = random.choice(PROXIES) if PROXIES else None\n",
    "            # kwargs = {\"proxy\": proxy} if proxy else {}\n",
    "\n",
    "            async with session.get(url, headers=headers, timeout=10) as response:\n",
    "                if response.status != 200:\n",
    "                    raise Exception(f\"Blocked with status {response.status}\")\n",
    "                if 'text/html' in response.headers.get('Content-Type', ''):\n",
    "                    return await response.text()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Fetch failed ({attempt + 1}/{retries}) {url}: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt + random.random())  # exponential backoff\n",
    "\n",
    "    return None  # after all retries fail\n",
    "\n",
    "\n",
    "def extract_links(html, base_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = urljoin(base_url, a['href'])\n",
    "        if is_same_domain(base_url, href):\n",
    "            links.add(href.split('#')[0])  # Remove fragments\n",
    "    return links\n",
    "\n",
    "def is_same_domain(base, target):\n",
    "    return urlparse(base).netloc == urlparse(target).netloc\n",
    "\n",
    "def is_product_url(url):\n",
    "    return any(re.search(pattern, url) for pattern in PRODUCT_PATTERNS)\n",
    "\n",
    "async def crawl_domain(domain):\n",
    "    visited = set()\n",
    "    to_visit = set([domain])\n",
    "    product_urls = set()\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        while to_visit and len(visited) < MAX_PAGES:\n",
    "            url = to_visit.pop()\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            print(f\"[~] Crawling: {url}\")\n",
    "            html = await fetch(session, url)\n",
    "            if not html:\n",
    "                continue\n",
    "\n",
    "            links = extract_links(html, url)\n",
    "            for link in links:\n",
    "                if link not in visited:\n",
    "                    if is_product_url(link):\n",
    "                        print(f\"[+] Found product: {link}\")\n",
    "                        product_urls.add(link)\n",
    "                    to_visit.add(link)\n",
    "\n",
    "    results[urlparse(domain).netloc] = sorted(product_urls)\n",
    "\n",
    "async def main():\n",
    "    tasks = [crawl_domain(domain) for domain in SEED_DOMAINS]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(\"product_urls.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(\"\\n✅ Done! Product URLs saved to 'product_urls.json'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetch HTML from the given endpoint\n",
    "url = \"https:#www.westside.com/collections/polo-t-shirts-for-men\"\n",
    "\n",
    "async def fetch_html():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, url)\n",
    "        if html:\n",
    "            print(\"HTML fetched successfully!\")\n",
    "            return html\n",
    "        else:\n",
    "            print(\"Failed to fetch HTML.\")\n",
    "            return None\n",
    "\n",
    "# Run the fetch_html coroutine\n",
    "html_content = await fetch_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin, urlparse\n",
    "async def extract_filtered_links(content, base_url, filtered_link):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    for tag in soup.find_all(href=True):\n",
    "        if tag.name == \"link\":\n",
    "            continue\n",
    "        link = tag.get(\"href\", None)\n",
    "        if link is None:\n",
    "            continue\n",
    "        if link.startswith(\"/\"):\n",
    "            link = urljoin(base_url, link)\n",
    "        if urlparse(link).netloc == urlparse(base_url).netloc:\n",
    "            if link not in filtered_link:\n",
    "                filtered_link.append(link.split('?')[0])  # Remove fragments\n",
    "    return filtered_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "# Fetch HTML from the given endpoint\n",
    "# url = \"https://www.westside.com/collections/polo-t-shirts-for-men\"\n",
    "url = \"https://www.tatacliq.com/mens-clothing-casual-wear-t-shirts-polos/c-msh1116100\"\n",
    "# url = \"https://www.virgio.com/collections/the-party-edit\"\n",
    "# url = \"https://www.nykaafashion.com/women/westernwear/shirts/c/7623\"\n",
    "base_url = f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n",
    "filtered_link = []\n",
    "async def slow_scroll_to_bottom(page, step=500, delay=0.5):\n",
    "    scroll_count = 1\n",
    "    while True:\n",
    "        previous_height = step * scroll_count\n",
    "        await page.evaluate(f\"window.scrollBy(0, {step})\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        new_height = await page.evaluate(\"() => document.body.scrollHeight\")\n",
    "\n",
    "        content = await page.content()\n",
    "        await extract_filtered_links(content, base_url, filtered_link)\n",
    "        if previous_height > new_height:\n",
    "            break\n",
    "        scroll_count += 1\n",
    "        # if scroll_count > 10:\n",
    "        #     break\n",
    "\n",
    "async def scroll_to_bottom(page, scroll_delay=2.0, max_scrolls=50):\n",
    "    \"\"\"Scrolls to the bottom of the page to load dynamic content.\"\"\"\n",
    "    previous_height = await page.evaluate(\"() => document.body.scrollHeight\")\n",
    "    \n",
    "    for _ in range(max_scrolls):\n",
    "        await page.evaluate(f\"window.scrollTo(0, {int(previous_height*0.8)})\")\n",
    "        time.sleep(scroll_delay)\n",
    "        \n",
    "        new_height = await page.evaluate(\"() => document.body.scrollHeight\")\n",
    "        if new_height == previous_height:\n",
    "            break\n",
    "        previous_height = new_height\n",
    "\n",
    "async def fetch_rendered_html(url):\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=False)\n",
    "            page = await browser.new_page()\n",
    "            # async def block_requests(route, request):\n",
    "            #     if request.resource_type in [\"image\", \"stylesheet\", \"font\", \"media\"]:\n",
    "            #         await route.abort()\n",
    "            #     else:\n",
    "            #         await route.continue_()\n",
    "\n",
    "            # await page.route(\"**/*\", block_requests)\n",
    "            await page.goto(url, wait_until='domcontentloaded')\n",
    "\n",
    "            # Select using DOM query style\n",
    "            # script_text = await page.locator('script[type=\"application/ld+json\"]').first.inner_text()\n",
    "            \n",
    "            # print(\"Raw script content from DOM:\")\n",
    "            # print(script_text)\n",
    "\n",
    "            await page.wait_for_timeout(3000)\n",
    "            # await page.wait_for_selector(\"img\", timeout=10000)\n",
    "            await slow_scroll_to_bottom(page)\n",
    "\n",
    "        # Get all product elements\n",
    "            items = page.locator('a')\n",
    "\n",
    "            content = await page.content()\n",
    "            await browser.close()\n",
    "            return content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while launching the browser: {e}\")\n",
    "\n",
    "# Fetch the fully rendered HTML\n",
    "html_content = await fetch_rendered_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41,\n",
       " ['https://www.virgio.com/',\n",
       "  'https://www.virgio.com/collections/all',\n",
       "  'https://www.virgio.com/irl',\n",
       "  'https://www.virgio.com/know-your-size',\n",
       "  'https://www.virgio.com/sustainability',\n",
       "  'https://www.virgio.com/account',\n",
       "  'https://www.virgio.com/products/party-wear-maroon-embellished-midi-dress',\n",
       "  'https://www.virgio.com/products/party-wear-black-embellished-dolman-top',\n",
       "  'https://www.virgio.com/products/party-wear-black-embellished-midi-dress',\n",
       "  'https://www.virgio.com/products/party-wear-maroon-embellished-dolman-top',\n",
       "  'https://www.virgio.com/products/ruffle-romance-black-ruffle-midi-dress',\n",
       "  'https://www.virgio.com/products/papillon-charm-one-shoulder-maxi-dress',\n",
       "  'https://www.virgio.com/products/ruffle-romance-blue-ruffle-dress-with-slit',\n",
       "  'https://www.virgio.com/products/ruffle-romance-bell-sleeve-ruffle-dress',\n",
       "  'https://www.virgio.com/products/papillon-charm-butterfly-printed-shirt',\n",
       "  'https://www.virgio.com/products/ruffle-romance-long-sleeve-ruffle-dress',\n",
       "  'https://www.virgio.com/products/9to9-cotton-top-pants-co-ord',\n",
       "  'https://www.virgio.com/products/9to9-cotton-co-ords-set',\n",
       "  'https://www.virgio.com/products/rose-revival-boat-neck-floral-maxi-dress',\n",
       "  'https://www.virgio.com/products/ruffle-romance-asymmetrical-hem-frill-dress',\n",
       "  'https://www.virgio.com/products/ruffle-romance-green-lettuce-hem-ruffle-top',\n",
       "  'https://www.virgio.com/products/ruffle-romance-black-ruffle-dress-with-slit',\n",
       "  'https://www.virgio.com/products/ruffle-romance-green-ruffle-midi-dress',\n",
       "  'https://www.virgio.com/products/papillon-charm-butterfly-print-midi-dress',\n",
       "  'https://www.virgio.com/products/rose-revival-cotton-floral-maxi-dress',\n",
       "  'https://www.virgio.com/products/work-wear-china-waist-gathered-midi-dress',\n",
       "  'https://www.virgio.com/products/rose-revival-viscose-strappy-midi-dress',\n",
       "  'https://www.virgio.com/products/rose-revival-cotton-tiered-flared-dress',\n",
       "  'https://www.virgio.com/products/rose-revival-viscose-floral-midi-dress',\n",
       "  'https://www.virgio.com/products/ric-rac-lace-puff-sleeve-a-line-dress',\n",
       "  'https://www.virgio.com/collections/the-party-edit',\n",
       "  'https://www.virgio.com/pages/about-us',\n",
       "  'https://www.virgio.com/blogs/news',\n",
       "  'https://www.virgio.com/stores',\n",
       "  'https://www.virgio.com/pages/ambassador-program',\n",
       "  'https://www.virgio.com/pages/refund-policy',\n",
       "  'https://www.virgio.com/pages/terms-and-conditions',\n",
       "  'https://www.virgio.com/pages/privacy-policy',\n",
       "  'https://www.virgio.com/pages/user-policy',\n",
       "  'https://www.virgio.com/collections/the-party-edit',\n",
       "  'https://www.virgio.com/collections/the-party-edit'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_link), filtered_link\n",
    "# pattern_p_urls = [u for u in filtered_link if '/p/' in u]\n",
    "# len(pattern_p_urls), pattern_p_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common pattern group:\n",
      "https://www.nykaafashion.com/lp/about-us\n",
      "https://www.nykaafashion.com/lp/about-us\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# Sample list of URLs\n",
    "urls = filtered_link\n",
    "\n",
    "# Optional: Remove query params\n",
    "def clean_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.scheme + \"://\" + parsed.netloc + parsed.path\n",
    "\n",
    "# Step 1: Clean and group by pattern\n",
    "def get_pattern(url):\n",
    "    url = clean_url(url)\n",
    "    # Replace numeric parts with {id}\n",
    "    pattern = re.sub(r'\\d+', '{id}', url)\n",
    "    return pattern\n",
    "\n",
    "# Step 2: Build groups\n",
    "group_map = defaultdict(list)\n",
    "for url in urls:\n",
    "    pattern = get_pattern(url)\n",
    "    group_map[pattern].append(url)\n",
    "\n",
    "# Step 3: Find the most frequent group\n",
    "most_common_group = max(group_map.values(), key=len)\n",
    "\n",
    "# Output\n",
    "print(\"Most common pattern group:\")\n",
    "for u in most_common_group:\n",
    "    print(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m         browser.close()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mextract_json_ld\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://www.nykaafashion.com/women/westernwear/shirts/c/7623\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mextract_json_ld\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_json_ld\u001b[39m(url):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchromium\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/web_crawler/.venv/lib/python3.13/site-packages/playwright/sync_api/_context_manager.py:47\u001b[39m, in \u001b[36mPlaywrightContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m             \u001b[38;5;28mself\u001b[39m._own_loop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loop.is_running():\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[32m     48\u001b[39m \u001b[38;5;250m                \u001b[39m\u001b[33;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m             )\n\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[32m     53\u001b[39m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[32m     54\u001b[39m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgreenlet_main\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mError\u001b[39m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "def extract_json_ld(url):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        page.goto(url, wait_until='networkidle')\n",
    "\n",
    "        # Locate all <script type=\"application/ld+json\"> tags\n",
    "        scripts = page.locator('script[type=\"application/ld+json\"]')\n",
    "        count = scripts.count()\n",
    "\n",
    "        print(f\"Found {count} JSON-LD script(s).\")\n",
    "\n",
    "        for i in range(count):\n",
    "            raw_json = scripts.nth(i).inner_text()\n",
    "            try:\n",
    "                data = json.loads(raw_json)\n",
    "                print(json.dumps(data, indent=2))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping script {i} - not valid JSON.\")\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "# Example usage\n",
    "extract_json_ld('https://www.nykaafashion.com/women/westernwear/shirts/c/7623')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
